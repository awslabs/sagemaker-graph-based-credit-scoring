{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c0c848",
   "metadata": {},
   "source": [
    "# Graph Neural Nets for Credit Scoring\n",
    "\n",
    "**Note**: This is a stripped down version of the SageMaker JumpStart Solution titled \"Graph-Based Credit Scoring\" for academic/education purposes. Please use the production version for purposes other than review, as this version is not as performant and does not use all SageMaker resources as the solution publicly released in JumpStart. For a brief overview, please see the related [blog](https://aws.amazon.com/blogs/machine-learning/build-a-corporate-credit-ratings-classifier-using-graph-machine-learning-in-amazon-sagemaker-jumpstart/). \n",
    "\n",
    "Credit scoring using just tabular data may be improved by exploiting data on business linkages, for which you may construct a graph, denoted as `CorpNet` -- for corporate network. You can then apply graph machine learning classification using GNNs on this graph and a tabular feature set for the nodes, to build a better ML model by further exploiting the information in network relationships. \n",
    "\n",
    "Graph Neural Nets (GNNs) are emerging as an interesting way to enhance existing machine learning models in several domains, see: https://www.amazon.science/blog/amazon-at-wsdm-the-future-of-graph-neural-networks\n",
    "\n",
    "#### If you run this notebook locally, please run with the default `Python3` kernel.\n",
    "\n",
    "#### If you run this notebook in Sagemaker Studio, please choose the below settings. \n",
    "- Kernel: `PyTorch 1.10 Python 3.8 GPU Optimized` \n",
    "- Use GPU machine: `ml.g4dn.xlarge` (4 core CPU, 1 GPU, 16 GB RAM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e062b",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "Credit ratings are traditionally generated using models that use financial statement data and market data, which is typically in tabular form, i.e., numeric and categorical data. This model constructs a network of firms using text from SEC filings and shows that using the network of firm relationships with tabular data can generate more accurate rating predictions. The model demonstrates a methodology to use big data to extend tabular data credit scoring models, which have been used by the ratings industry for decades, to the class of machine learning models on networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908d665",
   "metadata": {},
   "source": [
    "## What does this notebook do? \n",
    "\n",
    "This notebook (and related auxilliary program and data files) demonstrate the following: \n",
    "\n",
    "- *Network data*: The solution shows how to construct a network of connected companies using the MD&A section from SEC 10-K/Q filings. We assume that companies with similar forward looking statements are likely to be connected for credit events, though this is but one approach to constructing a corporate graph. These connections are represented in a graph. The solution provides a single function that generates the network. \n",
    "\n",
    "- *Tabular data*: For graph node features, the solution uses the variables in the Altman Z-score model and the industry category of each firm. These are provided in a synthetic dataset made available for demonstration purposes.\n",
    "\n",
    "- The graph data and tabular data are used to fit a rating classifier using graph neural nets (GNNs). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb64bc1",
   "metadata": {},
   "source": [
    "## Part 1: Constructing a Corporate Network using SEC Filings (`CorpNet`)\n",
    "\n",
    "- A simple approach is used to construct a network of credit connected companies using the text in the Management Discussion & Analysis (MD&A) section of the 10-K/Q filings, which are annual/quarterly reports companies are required to file with the SEC. The MD&A section usually contains the forward looking outlook for a company and also discusses trends and market conditions that may impact a firm. \n",
    "\n",
    "- To obtain the filings for each company that populates the `CorpNet`, use the AWS SageMaker JumpStart card titled \"Financial TabText Data Construction\". A blog showing how to do this is also available: https://aws.amazon.com/blogs/machine-learning/create-a-dashboard-with-sec-text-for-financial-nlp-in-amazon-sagemaker-jumpstart/. This data has already been collected and provided in the file `text_data.csv`.\n",
    "\n",
    "- The graph is based on the distribution of cosine similarities between document embeddings of the MD\\&A section of firms' 10-K/Q filings. The similarity of two firms is given by the following equation: $s = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}$, where $A, B$ are document embeddings. Since these embedding vectors may contain negative values, it is possible to return negative values. However, links in the network are based on values where $s>0.5$. The distribution of pairwise cosine similarities is shown below and the cutoff used is deep in the right tail. However, this is without loss of generality, and other cutoffs may be used to modulate the number of links in the network. \n",
    "\n",
    "![cosine_sims_synthdata](https://sagemaker-solutions-prod-us-east-2.s3.us-east-2.amazonaws.com/sagemaker-graph-neural-net-models-for-financial-classification/1.1.0/docs/cosine_sims_synthdata.png)\n",
    "\n",
    "\n",
    "- The degree distribution, i.e., the number of connections of each node in the graph, turns out to be power-law distributed, which is classic in small-world [scale-free](https://en.wikipedia.org/wiki/Scale-free_network) graphs, hence `CorpNet` appears to fall into this category of graphs. \n",
    "\n",
    "![degree_dist](https://sagemaker-solutions-prod-us-east-2.s3.us-east-2.amazonaws.com/sagemaker-graph-neural-net-models-for-financial-classification/1.1.0/docs/degree_dist.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79ea0a",
   "metadata": {},
   "source": [
    "## Part 2: The synthetic dataset of credit scoring features\n",
    "\n",
    "The model used is a well-known bankruptcy prediction approach, from the seminal paper by Altman (1968). A brief summary is provided here. See also the description at this [site](https://www.creditguru.com/index.php/bankruptcy-and-insolvency/altman-z-score-insolvency-predictor). Similar information is also available on many other sites.\n",
    "\n",
    "The model uses 8 inputs converted into 5 financial ratios: (i) Current assets ($CA$); (ii) Current liabilities ($CL$); (iii) Total Liabilities ($TL$); (iv) earnings before interest and taxes ($EBIT$); (v) Total Assets ($TA$); (vi) Net Sales ($NS$); (vii) Retained earnings ($RE$); and (viii) Market value of equity ($MVE$). The relationships between these 8 items are modeled as stipulated in the Altman model through the following five ratios: \n",
    "- A: EBIT / Total Assets \n",
    "- B: Net Sales / Total Assets\n",
    "- C: Mkt Value of equity / Total Liabilities\n",
    "- D: Working Capital / Total Assets\n",
    "- E: Retained Earnings / Total Assets\n",
    " \n",
    "The Z-score is then provided by the following formula, estimated using Linear Discriminant Analysis (LDA) and the coefficients in the formula are published widely by Altman.\n",
    "\n",
    "$$\n",
    "Zscore = 3.3 A + 0.99 B + 0.6 C + 1.2 D + 1.4 E\n",
    "$$\n",
    "\n",
    "High quality companies have high Z-scores and low quality companies have low ones. The simulated Z-scores used to construct our synthetic dataset are calibrated to broad balance-sheet, income-statement, and market data using averages for the U.S. economy from the following sources: \n",
    "\n",
    "- [Balance sheet data](https://fred.stlouisfed.org/release/tables?rid=434&eid=196197)\n",
    "- [Income statement data](https://fred.stlouisfed.org/release/tables?rid=434&eid=195208)\n",
    "- [Price to book](http://pages.stern.nyu.edu/~adamodar/New\\_Home\\_Page/datafile/pbvdata.html)\n",
    "\n",
    "The numbers taken from these sources are used to extract total values for US companies for the 8 inputs mentioned above. In addition, we obtain the average price-to-book ($P2B$) ratio for US companies, book equity value ($EQ$) and then generate market value of equity ($MVE$) as $P2B(EQ+RE)$. Working capital ($WC$) is as usual, $CA-CL$, the difference between current assets and current liabilities. We normalized all simulated financial data by Total Assets. This normalization does not impact the Altman ratios in any way. \n",
    "\n",
    "This dataset of simulated companies and their financials is then enhanced with a column of ratings, where the ratings are mapped -- with some noise -- to the Z-score, such that higher Z-score firms are assigned higher quality ratings and lower Z-score firms get lower quality ratings. The dataset titled `tabular_data.csv` is also provided with this notebook. \n",
    "\n",
    "Given that the `CorpNet` graph is generated from real SEC filings, we compute \"quality\" scores for each MD&A section using JumpStart APIs for NLP scoring, see the JumpStart card titled \"NLP Score Dashboard for SEC Text\" and the related documentation [link](https://sagemaker-jumpstart-industry-pack.readthedocs.io/en/latest/smjsindustry.nlp_scorer.html). The quality scores are a net sum of positive NLP scores (positivity, polarity, safe, sentiment, certainty) and negative NLP scores (fraud, litigious, negative, uncertainty, risk). These quality scores are used to associate higher \"quality\" nodes with higher ratings firms in the synthetic tabular dataset. This injects a connection between `CorpNet` and the synthetic data.\n",
    "\n",
    "We used the (MD&A) section of companies' SEC 10-K/Q filings to construct a corporate graph, `CorpNet`, already discussed above. The approach is based on the idea that companies facing similar risks will evidence similarities in their forward-looking statements in the MD&A section. Therefore, we construct a graph where each node is a company in the dataset and two companies are linked if the cosine similarity of their MD&A document embeddings is greater than 0.5. The notebook that accompanies this solution contains this generated graph data in addition to the tabular data described above. \n",
    "\n",
    "Therefore, we generate both, a graph using real SEC filings and associate it with the synthetic tabular data generated to match broad financial averages for US firms. \n",
    "\n",
    "**References**\n",
    "\n",
    "- Altman, Edward I. “Financial Ratios, Discriminant Analysis and the Prediction of Corporate Bankruptcy.” The Journal of Finance 23, no. 4 (1968): 589–609. https://doi.org/10.1111/j.1540-6261.1968.tb00843.x.\n",
    "\n",
    "- Altman, Edward I. “A Fifty-Year Retrospective on Credit Risk Models, the Altman Z -Score Family of Models and Their Applications to Financial Markets and Managerial Strategies.” Journal of Credit Risk 14, no. 4 (December 11, 2018): 1–34.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41aa5d4",
   "metadata": {},
   "source": [
    "## Data Summary\n",
    "\n",
    "1. The dataset has tabular data such as various accounting ratios (numerical) and industry codes (categorical). The dataset has $N = 3286$ rows. Rating labels are also added. These are the node features to be used with graph machine learning. \n",
    "\n",
    "2. The dataset also contains a corporate graph, which is undirected and unweighted. This solution also allows the user to tweak the structure of the graph by varying the way in which links are included. \n",
    "\n",
    "3. Classification using GNNs can be multi-category for all ratings or binary, divided between investment grade (rating levels AAA, AA, A, BBB) and non-investment grade (rating levels BB, B, CCC, CC, C, D). D=defaulted.  \n",
    "\n",
    "Next we move on to using the graph and tabular data to demonstrate how GNNs are applied to this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcc11e",
   "metadata": {},
   "source": [
    ">**<span style=\"color:RED\">Important</span>**: \n",
    ">This solution is for demonstrative purposes only. It is not financial advice and should not be relied on as financial or investment advice. The associated notebooks, including the trained model, use synthetic data, and are not intended for production use. While text from SEC filings is used, the financial data is synthetically and randomly generated and have no relation to any company's true financials. Hence, the synthetically generated ratings also do not have any relation to a company's true rating. You are free to adapt this solution for your own use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f1e95",
   "metadata": {},
   "source": [
    "This solution relies on a config file to run the provisioned AWS resources. Run the cell below to generate that file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fc797",
   "metadata": {},
   "source": [
    "## Part 3: Load up the required packages and data\n",
    "\n",
    "The solution uses several open source packages below and you should ensure that you comply with the terms of use of these packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ae916",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -I gensim bokeh networkx sagemaker scikit-learn matplotlib torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801da496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import construct_network_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06259a6c",
   "metadata": {},
   "source": [
    "## Tabular Feature Set\n",
    "\n",
    "We read in a file that contains all the data. It has the 5 Altman financial ratios A, B, C, D, E described in Part 2 above, and the binary rating class, as well as one-hot encoding for the industry categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'binary_rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3177f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "rating_df = pd.read_csv('datasets/tabular_data.csv')\n",
    "\n",
    "rating_df[target_column] = rating_df['Rating'].apply(lambda x: 1 if x in ['AAA', 'AA', 'A', 'BBB'] else 0)\n",
    "rating_df['Rating'] = le.fit_transform(rating_df['Rating'])\n",
    "\n",
    "rating_df.reset_index(inplace=True, drop=True)\n",
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad667486",
   "metadata": {},
   "source": [
    "Finally, we prune the tabular feature set, by removing the label column `Rating`, which is for multiclass rating. This dataset still includes the label column `binary_rating` for binary classification, which is the use case we develop here. If multicategory classification is required, then we drop `binary_rating` instead and keep the `Rating` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d25d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.drop(['Rating'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035895f-fb9f-44b8-966c-ef94c0e8014a",
   "metadata": {},
   "source": [
    "We now examine how much of the dataset has label 1 verus label 0. We see that the dataset is quite balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62902e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rating_df[target_column].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dab03",
   "metadata": {},
   "source": [
    "## Implementing Graph Construction\n",
    "\n",
    "In this section we use our custom approach to construct `CorpNet` using the text from SEC filings.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82dfd3",
   "metadata": {},
   "source": [
    "Read the text data to construct network data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d30908",
   "metadata": {},
   "source": [
    "#### * Due to the github file size limit, we split our dataset into 9 files, each of which has size < 25 MB. In the below cell, we read all files and concatenate into a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58263d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "dfs = [pd.read_csv(f, header=0) for f in glob.glob('datasets/dataset_*.csv')]\n",
    "text_df = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f32173",
   "metadata": {},
   "source": [
    "### Construct the network\n",
    "1. For each source node, i.e., a row in the dataset, find its neighbor nodes sorted from nearest to furthest based on some distance measurement. The distance measurement in this example is computed based on the embeddings of a text column **MDNA** generated from the **Doc2Vec** method. However, you may use embeddings from any algorithm, such as BERT, or financial embeddings from a pre-trained [RoBERTa-SEC model](https://aws.amazon.com/blogs/machine-learning/use-pre-trained-financial-language-models-for-transfer-learning-in-amazon-sagemaker-jumpstart/). \n",
    "\n",
    "2. For each source node, add a link if the document vector for a node has high cosine similarity with another node. Note that it is possible to generate zero links for some nodes. However, these nodes are still included in the graph. \n",
    "\n",
    "3. Add links, i.e., destination nodes for all nodes that are close in embedding space. \n",
    "\n",
    "4. The length of the `src` and `dst` lists will be the number of links in the network. They should both have the same length. \n",
    "\n",
    "5. The number of source nodes with links will differ from the number of destination nodes with links. Some nodes will be isolated, i.e., not linked at all. Both these numbers will be less than $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b39ccb",
   "metadata": {},
   "source": [
    "#### Drop isolated nodes and renumber all nodes\n",
    "\n",
    "We will use the Deep Graph Library ([DGL](https://www.dgl.ai/)) for graph machine learning. DGL requires that all nodes are consecutively numbered, and the code below ensures no missing node numbers in a sequence of nodes, so all nodes are numbered consecutively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "src_dst_dict = construct_network_data(text_df, text_column_name=\"MDNA\", embedding_size=300, cutoff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_nodes = set(rating_df.index) - (set(src_dst_dict['src'] + src_dst_dict['dst']))\n",
    "\n",
    "rating_df.drop(list(del_nodes), inplace=True)\n",
    "rating_df['node'] = rating_df.index\n",
    "rating_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270178ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map = {v:k for k,v in rating_df['node'].to_dict().items()}\n",
    "\n",
    "src_dst_dict['src'] = [node_map[n] for n in src_dst_dict['src']]\n",
    "src_dst_dict['dst'] = [node_map[n] for n in src_dst_dict['dst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae95425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "print('Highest index =', max(rating_df.index))\n",
    "print('# nodes =', len(rating_df))\n",
    "print('# source nodes with links =', len(set(src_dst_dict['src'])))\n",
    "print('# destination nodes with links =',len(set(src_dst_dict['dst'])))\n",
    "print('# Linked nodes =', len(set(src_dst_dict['src'] + src_dst_dict['dst'])))\n",
    "print('# isolated nodes =', len(rating_df) - len(set(src_dst_dict['src'] + src_dst_dict['dst'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f704c30",
   "metadata": {},
   "source": [
    "This completes the preparation of the corporate graph, denoted as `CorpNet`. We save the source and destination nodes to store `CorpNet`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3475016",
   "metadata": {},
   "source": [
    "## Part 4: Train a baseline model without graph machine learning\n",
    "\n",
    "It is useful to set a baseline that mimics a model that only uses the tabular data for each firm. This will enable a comparison of models that gain from enhancing the features used with graph data. In order to be agnostic to the baseline model and to set a competitive baseline, AutoGluon Tabular using a SageMaker AutoGluon Container is used to fit ML models the tabular data. AutoGluon has been shown to beat most other AutoML packages in kaggle competitions, discussed in this [blog](https://aws.amazon.com/blogs/opensource/machine-learning-with-autogluon-an-open-source-automl-library/). It uses stack-ensembling over a wide range of models and exploits the tabular feature set comprehensively. In contrast, for graph ML we use the [deep graph library (DGL)](https://www.dgl.ai/) that only fits one GNN which may be optimally chosen with hyperparmater optimization and will likely only beat AutoGluon in accuracy if there is sufficient information in the graph that adds value to the classifier. \n",
    "\n",
    "The next few blocks of code will train a rating classifier on the tabular features in this solution. \n",
    "\n",
    "Begin by uploading the train and test data to a S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208eccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(rating_df, test_size=0.2, random_state=46, stratify=rating_df[target_column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb12562",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d616d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ag_processed_data\n",
    "ag_train_df = train_df.drop([\"node\"], axis=1)\n",
    "ag_test_df = test_df.drop([\"node\"], axis=1)\n",
    "ag_train_df.to_csv(\"ag_processed_data/train_data.csv\", header=True, index=False)\n",
    "ag_test_df.to_csv(\"ag_processed_data/test_data.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4ad1b-9524-4383-af51-d3c8ad42d4a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install autogluon.tabular==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394758c-02da-4df3-9abe-ab79592c27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b3675",
   "metadata": {},
   "source": [
    "We run model training in an AutoGluon-Tabular. Recall that the classification task is a binary one, to predict whether a company is investment grade or below investment grade.\n",
    "\n",
    "The next block of code shows how simple the code is to run AutoGluon-Tabular in local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5c594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "target_column = \"binary_rating\"\n",
    "eval_metric = \"f1\"\n",
    "output_path_ag = \"ag-results\"\n",
    "\n",
    "train_data = TabularDataset(\"ag_processed_data/train_data.csv\")\n",
    "\n",
    "ag_predictor_args = {\n",
    "    \"label\": target_column,\n",
    "    \"path\": output_path_ag,\n",
    "    \"eval_metric\": eval_metric,\n",
    "\n",
    "}\n",
    "\n",
    "predictor = TabularPredictor(**ag_predictor_args).fit(train_data)\n",
    "\n",
    "logger.info(\"Best model: %s\", predictor.get_model_best())\n",
    "\n",
    "# Leaderboard\n",
    "lb = predictor.leaderboard()\n",
    "lb.to_csv(f'{output_path_ag}/leaderboard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35ffc7",
   "metadata": {},
   "source": [
    "### Perform inference on the model trained by AutoGluon Model\n",
    "\n",
    "We assess the performance of the tabular model on the test data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680eb41b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = TabularPredictor.load(model_dir)\n",
    "globals()[\"column_names\"] = predictor.feature_metadata_in.get_features()\n",
    "\n",
    "ag_pred_prob = predictor.predict_proba(ag_test_df.drop([target_column], axis=1)).values\n",
    "ag_pred_prob = np.array(ag_pred_prob)\n",
    "ag_pred = np.where(ag_pred_prob[:,1] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735dd089-def7-4146-bca6-ca49a6d5a490",
   "metadata": {},
   "source": [
    "### Compute evaluation metrics on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report, matthews_corrcoef, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34680a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_y_true = ag_test_df[target_column].values\n",
    "metrics_results = classification_report(ag_y_true, ag_pred, zero_division=1, output_dict=True)\n",
    "ag_results = pd.DataFrame(\n",
    "    {\n",
    "        \"F1 Score\": metrics_results[\"1\"][\"f1-score\"],\n",
    "        \"ROC AUC\": roc_auc_score(ag_y_true, ag_pred_prob[:, 1]),\n",
    "        \"Accuracy\": metrics_results[\"accuracy\"],\n",
    "        \"MCC\": matthews_corrcoef(ag_y_true, ag_pred),\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(ag_y_true, ag_pred),\n",
    "        \"Precision\": metrics_results[\"1\"][\"precision\"],\n",
    "        \"Recall\": metrics_results[\"1\"][\"recall\"],        \n",
    "    },\n",
    "    index=[\"AutoGluon\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f36d9",
   "metadata": {},
   "source": [
    "## Part 5: A Brief Introduction to Graph Machine Learning\n",
    "\n",
    "We assume that you have some knowledge of graph theory. The summary below is a brief description of graph ML. \n",
    "\n",
    "1. Machine learning using graph data is a broad subject and good expositions are available in books by Hamilton (2020); Blumauer and Nagy (2020); Ma and Tang (2021). (References below.) \n",
    "\n",
    "2. Graph machine learning requires representing the information on graphs as graph embeddings, of which a particular type is node embeddings. We can use deep learning methods to construct quality node embeddings using algorithms like [node2vec](https://snap.stanford.edu/node2vec/). You may also construct embeddings for links in a graph and also embeddings for entire graphs. \n",
    "\n",
    "3. Neural networks are used for classification with graph convolutional network (GCN) layers that use the normalized graph adjacency matrix with activation functions (e.g., ReLU) to incorporate network relationships into deep graph learning (Kipf and Welling, 2017). \n",
    "\n",
    "4. A graph (or network) $G$ comprises nodes or vertexes (set $V$, of size $|V|=n$) and links or edges (set $E$, of size $|E|=m$) and hence may be written as $G(V,E)$. The edge set may be (i) directed or undirected, (ii) weighted or unweighted. The graph may have single node types and single edge types, in which case it is denoted as \"homogeneous\" graph, but if it has multiple types of nodes or links, then it is known as a \"heterogeneous\" graph. For example, in a lending network, some nodes may be borrowers, others lenders, and some both. The number of links each node has is known as the \"degree\" of the node, and the distribution of number of links across all nodes is called the \"degree distribution\" of the graph, shown earlier. Our solution here is based on nodes that are of one type, companies, and hence uses a homogeneous graph.  \n",
    "\n",
    "5. The links in a homogeneous graph are often represented by an \"adjacency matrix\" $A$ of size $n \\times n$, which represents all links in the graph with non-zero values, such that a link from node $i$ to node $j$ is designated by $A_{ij}>0$. For an undirected graph, matrix $A$ is symmetric, i.e., $A_{ij}=A_{ji}$. An alternate data structure for a graph is a links table $L$ of size $m \\times 3$, with one row for each link from node $i$ to node $j$ with edge weight $w$, which we can denote as $L(i,j,w)$. The links table is a parsimonious data structure compared to the adjacency matrix, and for large (and sparse) graphs, is much more memory efficient.\n",
    "\n",
    "6. Node representations (embeddings) are vectors in $d$-dimensional embedding space. Algorithms to generate these embeddings aim to map similarity of nodes in the graph to closeness in the embedding vector space. Let the $d$-dimensional embedding vectors for nodes $i,j$ be denoted as $z_i, z_j \\in {\\cal R}^d$. We try to find vectors that map into the values in the adjacency matrix $A$, i.e., minimize loss ${\\cal L} = \\sum_i \\sum_j \\parallel z_i^\\top \\cdot z_j - A_{ij} \\parallel^2$.\n",
    "\n",
    "7. GCNs are trained to produce node, link, and graph embeddings. Node embeddings are a function of the features set of the node and its nearest neighbors, which may be a single hop away to many hops away (Cao, Lu, Xu (2015); Wu et al (2019)), a modeling choice, and have fixed length, dimension $d$. See Perozzi et al. (2014) for an efficient approach, known as `DeepWalk`, improved upon with biased random walks in the node2vec algorithm of Grover and Leskovec (2016). \n",
    "\n",
    "8. These *shallow* embeddings discussed above ignore data available on node features. They also cannot generate embeddings for new nodes that were not part of the original training of node embeddings. Better {\\it deep} encodings for nodes is possible with deep learning, using graph neural nets (GNNs). For a detailed exposition, see Scarselli et al (2009), a brief overview is as follows. Assume $k$ features at each of the $n$ nodes, resulting in a feature table $X \\in {\\cal R}^{k \\times n}$, with vector $x_i$ being the feature vector for node $i$. In our setting, these are all the variables that define the credit risk of a firm. The approach to generate embeddings is as follows. Assume a fixed neighborhood of a node (say 3 hops) and look at the 3-level tree below the chosen node. Suppose the node $i$ is connected to 4 nodes, each of which is connected to $1,2,4,3$ nodes respectively, and each of these nodes is also connected to more nodes (the exact number of connections is not important and the numbers here are merely illustrative, these links may contain cycles and thus include recursion). Assuming we have a representation for each of the 4 nodes connected to node $i$ already, then we can average these vectors, feed them into a neural network ${\\cal N}$ where the network generates a final output of embedding size $d$ as a node embedding for node $i$. Note that this approach from one hop away, can be recursively extended to the next 2 hops as well, ending with (in this case) 3 hops away (the leaves of the tree are the 3-hop nodes). At the last level, the inputs are the $k$-dimensional feature vectors for each node. These get transformed by weighting matrices in the vectors as we go up the tree to end up with the final embedding for node $i$. The node-level generated embeddings can be used with loss functions for similarity as discussed before in unsupervised mode, or on a node classification task in supervised mode. Once the neural net has been trained, you may generate embeddings for new nodes as they are added to the graph. This was not possible with the simpler embeddings discussed in the preceding paragraph.  \n",
    "\n",
    "9. So far, input vectors from the lower levels of a subtree emanating from a target node have been aggregated by weighted averaging of the inputs of neighbor nodes along with the embedding of the target node. Here, we use DGL, which implements a node embedding scheme known as [GraphSAGE](http://snap.stanford.edu/graphsage/). This approach extends neighborhood aggregation to concatenating the target node embedding with a function of the neighborhood node embeddings that may be simply a mean (pooling), or other forms of pooling, as well as applying more complex activations such as LSTMs using graph convolutional networks (GCNs). We will use these GCNs below. \n",
    "\n",
    "10. The embeddings and node features are now used to learn a classification neural net. \n",
    "\n",
    "### References\n",
    "\n",
    "- Cao, Shaosheng, Wei Lu, and Qiongkai Xu. “GraRep: Learning Graph Representations with Global Structural Information.” In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, 891–900. CIKM ’15. New York, NY, USA: Association for Computing Machinery, 2015. https://doi.org/10.1145/2806416.2806512.\n",
    "- Grover, Aditya, and Jure Leskovec. “Node2vec: Scalable Feature Learning for Networks.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 855–64. KDD ’16. New York, NY, USA: Association for Computing Machinery, 2016. https://doi.org/10.1145/2939672.2939754.\n",
    "- Hamilton, William L. Graph Representation Learning. San Rafael, California: Morgan & Claypool, 2020.\n",
    "- Blumauer, Andreas, and Helmut Nagy. The Knowledge Graph Cookbook. edition mono/monochrom, 2020.\n",
    "- Ma, Yao, and Jiliang Tang. Deep Learning on Graphs. Cambridge: Cambridge University Press, 2021.\n",
    "- Kipf, Thomas N., and Max Welling. “Semi-Supervised Classification with Graph Convolutional Networks.” In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. https://openreview.net/forum?id=SJU4ayYgl.\n",
    "- Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. “DeepWalk: Online Learning of Social Representations.” In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 701–10. KDD ’14. New York, NY, USA: Association for Computing Machinery, 2014. https://doi.org/10.1145/2623330.2623732.\n",
    "- Scarselli, F., M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini (2009, January). \"The graph neural\n",
    "network model.\" IEEE transactions on neural networks 20 (1), 61–80. https://dl.acm.org/doi/10.1109/TNN.2008.2005605\n",
    "- Wu, Felix, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. “Simplifying Graph Convolutional Networks.” ArXiv:1902.07153 [Cs, Stat], June 20, 2019. http://arxiv.org/abs/1902.07153."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2bf62",
   "metadata": {},
   "source": [
    "## Part 6: Train a Graph Convolutional Network with DGL using SageMaker Pytorch Container\n",
    "\n",
    "We use an adaptation of the GraphSAGE model (http://snap.stanford.edu/graphsage/) in conjunction with the Deep Graph Library (DGL, https://www.dgl.ai/). The steps are as follows:\n",
    "\n",
    "1. Read in graph data from S3 and create the source and destination node lists for `CorpNet`. \n",
    "2. Read in the graph node feature sets (train and test).\n",
    "3. Set tunable hyperparameters. \n",
    "4. Call the specialized graph machine learning container running PyTorch to fit the GNN without hyperparameter optimization (HPO).\n",
    "5. Redo graph ML with HPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e3821",
   "metadata": {},
   "source": [
    "Save the model to the local directory and upload them to your S3 bucket to be ready for predictive use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bf779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in graph data\n",
    "src_dst_df = pd.DataFrame.from_dict(src_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ca51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dst_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4815e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "col_to_normalization = [\"CurrentLiabs\", \"TotalLiabs\", \"RetainedEarnings\", \"CurrentAssets\", \"NetSales\", \"MktValueEquity\"]\n",
    "col_to_onehot_encoded = [\"industry_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb5c340",
   "metadata": {},
   "source": [
    "First, we normalize the numerical columns for both train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train2normalize = train_df[col_to_normalization].values\n",
    "scaler.fit(train2normalize)\n",
    "normalized_train = scaler.transform(train2normalize)\n",
    "train_df[col_to_normalization] = normalized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_test = scaler.transform(test_df[col_to_normalization].values)\n",
    "test_df[col_to_normalization] = normalized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cac64a",
   "metadata": {},
   "source": [
    "Next, one-hot encode the categorical column for both train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='error')\n",
    "train2enc = train_df[col_to_onehot_encoded].values\n",
    "enc.fit(train2enc)\n",
    "encoded_train = enc.transform(train2enc).toarray()\n",
    "\n",
    "enc_col_names = []\n",
    "for each in zip(col_to_onehot_encoded, enc.categories_):\n",
    "    enc_col_names += [each[0]+f\"_{x}\" for x in each[1]]\n",
    "train_df[enc_col_names] = encoded_train\n",
    "train_df.drop(col_to_onehot_encoded, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef49f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2enc = test_df[col_to_onehot_encoded].values\n",
    "encoded_test = enc.transform(test2enc).toarray()\n",
    "test_df[enc_col_names] = encoded_test\n",
    "test_df.drop(col_to_onehot_encoded, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p gcn_processed_data\n",
    "src_dst_df.to_csv(\"gcn_processed_data/src_dst_df.csv\", header=True, index=False)\n",
    "train_df.to_csv(\"gcn_processed_data/features_train.csv\", header=True, index=True) # make sure the index is saved for distinguishing the train, validation, and test data. For details, see GCN/train_dgl_pytorch_entry_point.py\n",
    "test_df.to_csv(\"gcn_processed_data/features_test.csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4b74a",
   "metadata": {},
   "source": [
    "### Set the initial hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"n_hidden\": 32,\n",
    "    \"n_layers\": 2,\n",
    "    \"dropout\": 0.0,\n",
    "    \"weight_decay\": 0.00001,\n",
    "    \"n_epochs\": 120,\n",
    "    \"lr\": 0.0179,\n",
    "    \"aggregator_type\": \"pool\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c116d61",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We run the model training using the open source Deep Graph Library (https://www.dgl.ai/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61610038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install dgl torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e578a-95dd-4afc-ae4e-601f402a4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn_local.train_dgl_pytorch_entry_point_local import entry_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceac6b-dbb1-4e16-afc5-de1f25c77774",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p gcn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54f37c-d2c1-47a7-bf63-a7d02e70039b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "entry_train(\n",
    "    training_dir = \"gcn_processed_data\",\n",
    "    model_dir = \"gcn_results\",\n",
    "    output_dir = \"gcn_results\",\n",
    "    features_train = \"features_train.csv\",\n",
    "    features_test = \"features_test.csv\",\n",
    "    target_column = target_column,\n",
    "    source_destination_node_index = \"src_dst_df.csv\",\n",
    "    n_hidden = hyperparameters[\"n_hidden\"],\n",
    "    n_layers = hyperparameters[\"n_layers\"],\n",
    "    dropout = hyperparameters[\"dropout\"],\n",
    "    weight_decay = hyperparameters[\"weight_decay\"],\n",
    "    n_epochs = hyperparameters[\"n_epochs\"],\n",
    "    lr = hyperparameters[\"lr\"],\n",
    "    aggregator_type = hyperparameters[\"aggregator_type\"],\n",
    "    predictions_file_name = \"predictions.csv\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199c87",
   "metadata": {},
   "source": [
    "### Read the Prediction Output for the Test Data\n",
    "The current training process is undertaken in a transductive setting where the features of the test dataset, not including the target column, are used to construct the graph and thus the test data are included in the training process. At the end of training, the predictions on the test dataset are generated and saved in the **output_location** in the s3 bucket. \n",
    "\n",
    "**Important**: Note that the labels of the test dataset are not used for training, and our exercise is aimed at predicting these labels using node embeddings for the test dataset nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gcn_output = pd.read_csv(os.path.join(\"gcn_results\", \"predictions.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, gcn_y_pred, gcn_y_pred_prob_class_1 = gcn_output[\"target\"], gcn_output[\"pred\"], gcn_output[\"pred_proba_class_1\"]\n",
    "metrics_gcn_results = classification_report(y_true, gcn_y_pred, zero_division=1, output_dict=True)\n",
    "\n",
    "gcn_results = pd.DataFrame(\n",
    "    {\n",
    "        \"F1 Score\": metrics_gcn_results[\"1\"][\"f1-score\"],\n",
    "        \"ROC AUC\": roc_auc_score(y_true, gcn_y_pred_prob_class_1),\n",
    "        \"Accuracy\": metrics_gcn_results[\"accuracy\"],\n",
    "        \"MCC\": matthews_corrcoef(y_true, gcn_y_pred),\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(y_true, gcn_y_pred),\n",
    "        \"Precision\": metrics_gcn_results[\"1\"][\"precision\"],\n",
    "        \"Recall\": metrics_gcn_results[\"1\"][\"recall\"],        \n",
    "    },\n",
    "    index=[\"GCN No HPO\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64348dc4",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The various performance metrics are shown below. We can see that the deep learning model cannot deliver an optimal performance with only one configuration of hyperparameters. Hyperparameter optimization (HPO) is necessary to systematically find the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58206883",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e7299",
   "metadata": {},
   "source": [
    "The results for AutoGluon do not use the graph, only the tabular data. When we add in the graph data and use hyperparameter optimization (HPO), we get a material gain in performance, from 73% F1 score to 81%. \n",
    "\n",
    "In the production version, we further extend this model with hyperparemeter optimization to get more accurate results, as shown in the paper. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
